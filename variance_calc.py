##################################################
## A SCRIPT TO EXTRACT HAPLTOTYPE INFORMATION FROM BAM FILES
##################################################
## FREE USE
##################################################
## Author: Philipp Wagner
## Copyright: Copyright 2022
## Version: 1.0.0
## Email: philipp.wagner@unibas.ch
## Status: dev
##################################################


import argparse
import pysam
import json
import sys, csv
import numpy as np
import random
from numpy import loadtxt
from Bio.Seq import Seq
import pandas as pd



#----------------------------------------------------------------------------------------
# TERMINAL UI
#----------------------------------------------------------------------------------------

# SET UP ARGUMETNS TO BE PASSED IN TERMINAL
parser = argparse.ArgumentParser(description="haplotype extractor",
                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)
parser.add_argument("input", help="input bam file")
# parser.add_argument("output", help="output filename")
parser.add_argument("-c", "--contig", type=str, help="reference name/contig")
parser.add_argument("-g", "--genbank_file_json", type=str, help="data_report generated by GenBank in json format (.jsonl)")
parser.add_argument("-s", "--snp_file", type=str, help="file with list of SNPs, one nucleotide position per line")
parser.add_argument("-q", "--min_quality", type=int, help="minimal quality score for base to qualify for haplotype")
parser.add_argument("--subset", nargs="*", type=int, default=[], help="process a subset of random reads (input a list of % values)")
parser.add_argument("--replicates", type=int, default=1, help="make replicatds of picking the subsets (requires subset to be set)")
parser.add_argument("-o", "--output", type=str, help="output filename")
args = parser.parse_args()
config = vars(args)
# print(config)
# print(args)

infile = args.input
outfile = args.output
datareport = args.genbank_file_json
snpfile = args.snp_file
contig = args.contig

makesubsets = args.subset
replicates = args.replicates

minqual = args.min_quality




#----------------------------------------------------------------------------------------
# READ FILES
#----------------------------------------------------------------------------------------

# input bam file containing reads to be analyzed
samfile = pysam.AlignmentFile(infile, "rb")

# load a list of nucleotide SNP positions on the reference
rawsnps = loadtxt(snpfile, comments="#", delimiter="\n", unpack=False)

with open(datareport) as f:
    gb = json.load(f)

#----------------------------------------------------------------------------------------
# LOOP THROUGH THE REGIONS OF INTEREST (ROI) / SNP
# and store the nucleotide of each read at the SNP site
#----------------------------------------------------------------------------------------


# IMPORTANT NOTICE:
# pysam uses half open, 0-based coordinates
# the SNPs are however stored with 1-based coordinates
# genbank

# TO APPLY INTRON CORRECTION
# exons full open (bis und mit), 0-based
# snps 1-based



# GET SNP LIST
if rawsnps.size == 0:
    raise Exception('No SNP sites found. Cannot estimate haplotypes.')
elif rawsnps.size == 1:
    # if only one snp is in the file, it is not read as an array leading to errors below
    rawsnps = np.array([rawsnps], dtype='int32')
else:
    rawsnps = np.array(rawsnps, dtype='int32')

# sort the snps
rawsnps = np.sort(rawsnps)




# GET ALL READS
allreads = samfile.fetch(contig)








# FILTER SNPS OCCURING IN EXONS
# and create a dictionary list with the exons and the snps occuring in the corresponding exon

lastend = -1
lastintroncorr = 0
snps = [] # should rather be called exons
ref_begin = np.int32(gb['transcripts'][0]['genomicRange']['range'][0]['begin']) # begin of the reference sequence

# Loop through exons
for ex in gb['transcripts'][0]['exons']['range']:

    # store begin and end positions
    exon = {
        'begin': int(ex['begin']) - ref_begin,
        'end': int(ex['end']) - ref_begin,
    }

    # find and store the snps in that exon
    exon['snps'] = rawsnps[np.logical_and(rawsnps-1 >= exon['begin'], rawsnps <= exon['end'])]

    # calculate intron correction (the sum of the length of all introns until the current exon)
    # apply -1 since exons begin and end are written in full open notation
    exon['intronCorr'] = exon['begin'] - lastend + lastintroncorr - 1
    lastend = exon['end']
    lastintroncorr = exon['intronCorr']

    # add the dictionary to the list
    snps.append(exon)


# Number of SNPs
snpsflat = [snp for exon in snps for snp in exon['snps']]
# snpcount = sum(snp['snps'].size for snp in snps)







# PREPARE RESULT ARRAY
res = {}
for read in allreads:
    res[read.query_name] = [{'triplet':['-','-','-'], 'fail': True, 'aa': '-'} for sub in range(len(snpsflat))]







# MAIN LOOP TO DETERMINE VARIANCES
# iterate through SNPs
index = 0
for exon in snps:
    for snppos in exon['snps']:

        # find triplet
        tripstart = snppos-1 - (snppos-1 - exon['intronCorr']) % 3
        tripi = 0
        
        # create iterable object from bam file
        roi = samfile.pileup(contig, start=tripstart, end=tripstart+3, truncate=True, irgnore_overlaps=False, min_base_quality=0)
        for col in roi:
#             print(col.pos)
            

            # ITERATE THROUGH ALL READS AT THIS SNP SITE
            for row in col.pileups:
                read = row.alignment
                
                # qualtiy check
                if isinstance(row.query_position, int) and read.query_qualities[row.query_position] > minqual:## apply quality filter and not empty filter
                    res[read.query_name][index]['triplet'][tripi] = read.query_sequence[row.query_position]
                else:
                    res[read.query_name][index]['triplet'][tripi] = '-'
                
                # we just added the last base to the triplet
                if tripi == 2:
                    if not next((base for base in res[read.query_name][index]['triplet'] if base == '-'), False):
                        if len(''.join(res[read.query_name][index]['triplet'])) < 3:
                            print(read.query_name, ''.join(res[read.query_name][index]['triplet']), col.pos)
                        res[read.query_name][index]['aa'] = Seq(''.join(res[read.query_name][index]['triplet'])).translate()
                        res[read.query_name][index]['fail'] = False
                    else:
                        res[read.query_name][index]['fail'] = True
            
            # increment index in triplet
            tripi += 1
        # increment snp index
        index += 1
        
        #debug
#         if snppos > 1:
#             break




# find reads where no SNP had a missing nucl or was below the minimal quality
suc = pd.DataFrame([[snp['fail'] for snp in read[1]] for read in res.items()], index=res, columns = list(map(str, snpsflat)))
nofail = suc.query(f'not `{ "` and not `".join(map(str, snpsflat)) }`').index


# create a data frame wiht all the successfully extracted haplotypes
trip = pd.DataFrame([[''.join(snp['triplet']) for snp in read[1]] for read in res.items()], index=res, columns = list(map(str, snpsflat)))
trip = trip.loc[nofail]
trip['hapl'] = trip.sum(axis=1)





# create data frame with amino acids instead of triplets
amino = pd.DataFrame([[str(snp['aa']) for snp in read[1]] for read in res.items()], index=res, columns = list(map(str, snpsflat)))
amino = amino.loc[nofail]
amino['hapl'] = amino.sum(axis=1)


#----------------------------------------------------------------------------------------
# OUTPUT THE RESULT AS CSV
#----------------------------------------------------------------------------------------


# count the haplotypes and export the data frame
trip.value_counts().rename('count').to_csv(outfile, sep='\t')
amino.value_counts().rename('count').to_csv('aa_' + outfile, sep='\t')


if makesubsets:
    for perc in makesubsets:
        for repl in range(0, replicates):
            trues = round(len(trip)*(perc/100)) 
            filtlist = [True]*trues + [False]*(len(trip)-trues)
            random.shuffle(filtlist)
            tmptrip = trip[filtlist]
            tmptrip.value_counts().rename('count').to_csv(f'{outfile}.{perc}.{repl}.sub', sep='\t')
            # tmptrip.value_counts().rename('count').to_csv(outfile + '.' + perc + '.sub', sep='\t')
