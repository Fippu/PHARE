##################################################
## A SCRIPT TO EXTRACT HAPLTOTYPE INFORMATION FROM BAM FILES
##################################################
## FREE USE
##################################################
## Author: Philipp Wagner
## Copyright: Copyright 2022
## Version: 1.0.0
## Email: philipp.wagner@unibas.ch
## Status: dev
##################################################


import argparse
import pysam
import json
import sys, csv
import numpy as np
import random
from numpy import loadtxt
from Bio.Seq import Seq
from Bio import SeqIO
# import pandas as pd



#----------------------------------------------------------------------------------------
# TERMINAL UI
#----------------------------------------------------------------------------------------

# SET UP ARGUMETNS TO BE PASSED IN TERMINAL
parser = argparse.ArgumentParser(description="haplotype extractor",
                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)
parser.add_argument("input", help="input bam file")
# parser.add_argument("output", help="output filename")
parser.add_argument("-c", "--contig", type=str, help="reference name/contig")
parser.add_argument("-g", "--genbank_file_json", type=str, help="data_report generated by GenBank in json format (.jsonl)")
parser.add_argument("-s", "--snp_file", type=str, help="file with list of SNPs, one nucleotide position per line")
parser.add_argument("-q", "--min_quality", type=int, help="minimal quality score for base to qualify for haplotype")
parser.add_argument("-r", "--reference", type=str, default='', help="reference fasta file with. Only required if --filter_silent is set")
parser.add_argument("--amplicon_start", default=-1, type=int, help="TODO")
parser.add_argument("--amplicon_end", default=10000000000, type=int, help="TODO")
args = parser.parse_args()
config = vars(args)
# print(config)
# print(args)

infile = args.input
datareport = args.genbank_file_json
snpfile = args.snp_file
contig = args.contig

reference = args.reference

minqual = args.min_quality




amplicon_start = args.amplicon_start
amplicon_end = args.amplicon_end



print(infile)
print(snpfile)

#----------------------------------------------------------------------------------------
# READ FILES
#----------------------------------------------------------------------------------------

# input bam file containing reads to be analyzed
samfile = pysam.AlignmentFile(infile, "rb")

# load a list of nucleotide SNP positions on the reference
rawsnps = loadtxt(snpfile, comments="#", unpack=False)

with open(datareport) as f:
    gb = json.load(f)




#----------------------------------------------------------------------------------------
# PREPARE A 2D ARRAY WITH ALL THE EXON AND SNP SITES
# (! caching)
#----------------------------------------------------------------------------------------

# IMPORTANT NOTICE:
# pysam uses half open, 0-based coordinates
# the SNPs are however stored with 1-based coordinates
# genbank

# TO APPLY INTRON CORRECTION
# exons full open (bis und mit), 0-based
# snps 1-based


print('Removing silent SNPs')



# GET SNP LIST
if rawsnps.size == 0:
    raise Exception('No SNP sites found. Cannot estimate haplotypes.')
elif rawsnps.size == 1:
    # if only one snp is in the file, it is not read as an array leading to errors below
    rawsnps = np.array([rawsnps], dtype='int32')
else:
    rawsnps = np.array(rawsnps, dtype='int32')

# sort the snps
rawsnps = np.sort(rawsnps)



# FILTER out SNPS OCCURING IN INTRONS
# and create a dictionary list with the exons and the snps occuring in the corresponding exon

lastend = -1
lastintroncorr = 0
snps = [] # should rather be called exons
ref_begin = np.int32(gb['transcripts'][0]['genomicRange']['range'][0]['begin']) # begin of the reference sequence

# Loop through exons
for ex in gb['transcripts'][0]['exons']['range']:

    # store begin and end positions
    exon = {
        'begin': int(ex['begin']) - ref_begin,
        'end': int(ex['end']) - ref_begin,
    }


    if exon['end'] < amplicon_start or exon['begin'] > amplicon_end:
        print('snp removed since outside of area', exon['end'] < amplicon_start, exon['end'], amplicon_start, exon['begin'] > amplicon_end, exon['begin'], amplicon_end)
        continue

    # THESE IFS CAN BOTH BE TRUE
    if exon['begin'] < amplicon_start:
        exon['begin'] = amplicon_start
    if exon['end'] > amplicon_end:
        exon['end'] = amplicon_end


    # find and store the snps in that exon
    exon['snps'] = rawsnps[np.logical_and(rawsnps-1 >= exon['begin'], rawsnps <= exon['end'])]

    # calculate intron correction (the sum of the length of all introns until the current exon)
    # apply -1 since exons begin and end are written in full open notation
    exon['intronCorr'] = exon['begin'] - lastend + lastintroncorr - 1
    lastend = exon['end']
    lastintroncorr = exon['intronCorr']

    # add the dictionary to the list
    snps.append(exon)





# Create a flat representation of the snps
snpsflat = [snp for exon in snps for snp in exon['snps']]







#----------------------------------------------------------------------------------------
# LOOP THROUGH THE REGIONS OF INTEREST (ROI) / SNP
# and store the nucleotide of each read at the SNP site
# (! caching)
#----------------------------------------------------------------------------------------

# GET ALL READS
allreads = samfile.fetch(contig)


# PREPARE RESULT ARRAY
res = {}
for read in allreads:
    res[read.query_name] = [{'triplet':['-','-','-'], 'fail': True, 'aa': '-', 'snp': -1} for sub in range(len(snpsflat))]


# MAIN LOOP TO DETERMINE VARIANCES
# iterate through SNPs
index = 0
for exon in snps:
    for snppos in exon['snps']:

        # find triplet
        tripstart = snppos-1 - (snppos-1 - exon['intronCorr']) % 3
        tripi = 0
        
        # create iterable object from bam file
        roi = samfile.pileup(contig, start=tripstart, end=tripstart+3, truncate=True, irgnore_overlaps=False, min_base_quality=0)
        for col in roi:
#             print(col.pos)
            

            # ITERATE THROUGH ALL READS AT THIS SNP SITE
            for row in col.pileups:
                read = row.alignment
                
                # qualtiy check
                if isinstance(row.query_position, int) and read.query_qualities[row.query_position] > minqual:## apply quality filter and not empty filter
                    res[read.query_name][index]['triplet'][tripi] = read.query_sequence[row.query_position]
                else:
                    res[read.query_name][index]['triplet'][tripi] = '-'
                
                # we just added the last base to the triplet
                if tripi == 2:
                    # check if there was a nucleotide that failed quality checks or was a gap
                    if not next((base for base in res[read.query_name][index]['triplet'] if base == '-'), False):
                        if len(''.join(res[read.query_name][index]['triplet'])) < 3:
                            raise Exception('Unexplained error: Triplet length under 3 nucleotides in:', read.query_name, ''.join(res[read.query_name][index]['triplet']), col.pos)
                        res[read.query_name][index]['aa'] = str(Seq(''.join(res[read.query_name][index]['triplet'])).translate())
                        res[read.query_name][index]['fail'] = False
                    else:
                        res[read.query_name][index]['fail'] = True
            
            # increment index in triplet
            tripi += 1
        # increment snp index
        index += 1
with open(infile + '.res' + '.json', 'w') as f:
    json.dump({'snps': list(map(int, snpsflat)), 'res': res}, f)


print(snps)
#----------------------------------------------------------------------------------------
# AN EXTRA STEP TO FILTER OUT SILENT MUTATITIONS
# this can increase sensitivity, because less SNPs have to fulfill the quality requirements
#----------------------------------------------------------------------------------------

# get the reference file
# get the reference sequence
refseq = False
if reference != '':
    for record in SeqIO.parse(reference, "fasta"):
        if record.id == contig:
            refseq = record.seq
            break
    if not refseq:
        raise Exception('The contig does not match any contig in the reference')



snp_blacklist = []
index = 0
for snppos in snpsflat:
    aacount = {}
    for read in res.values():
        if not read[index]['fail']:
            aacount[read[index]['aa']] = aacount.get(read[index]['aa'], 0) + 1
    # print(aacount)
    tot = sum(aacount.values())

    filtered_aacount = {k: v for k, v in aacount.items() if v/tot > 0.05}
    if len(filtered_aacount) == 1:
        # we only have one variant at this snip
        tripstart = snppos-1 - (snppos-1 - exon['intronCorr']) % 3
        if ( list(filtered_aacount.keys())[0] == str(Seq(''.join(refseq[tripstart:(tripstart+3)])).translate()) ):
            # the variant is identical to the reference. hence we want to remove it from the snp list
            snp_blacklist.append(snppos)

    index += 1
# remove all the snps in blacklist
# print(snp_blacklist)
snpsfiltered = [value for value in snpsflat if value not in snp_blacklist]


#----------------------------------------------------------------------------------------
# OUTPUT THE RESULT AS CSV
#----------------------------------------------------------------------------------------

# print(snpsites)
# print(dict(list(res.items())[0:50]))

with open(infile + '.non-silent.snps', 'w') as f:
    for pos in snpsfiltered:
        f.write(str(pos) + '\n')

# for pos in snpsites:
#     print(pos)